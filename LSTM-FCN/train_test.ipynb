{"cells":[{"cell_type":"markdown","metadata":{"id":"KE_mArRXDdgk"},"source":["## Multivariate LSTM-FCN\n","- Code source : https://github.com/metra4ok/MLSTM-FCN-Pytorch\n","- Modified some sturcture and parameters w.r.t. our IOT hackathon data"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/Mydrive')\n","%cd \"/content/Mydrive/MyDrive/IOT_hackathon/Github/IOT_hackathon\"\n","%cd \"/content/Mydrive/MyDrive/IOT_hackathon/Github/IOT_hackathon/LSTM-FCN\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k8yEJXViDmyY","executionInfo":{"status":"ok","timestamp":1661348028764,"user_tz":-540,"elapsed":18871,"user":{"displayName":"김당찬","userId":"13376466808736477022"}},"outputId":"f92e78e9-c768-4f14-aacf-fe4f1e8a66d9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/Mydrive\n","/content/Mydrive/MyDrive/IOT_hackathon/Github/IOT_hackathon\n","/content/Mydrive/MyDrive/IOT_hackathon/Github/IOT_hackathon/LSTM-FCN\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"qOTeJcKJDdgo","executionInfo":{"status":"ok","timestamp":1661348037409,"user_tz":-540,"elapsed":4845,"user":{"displayName":"김당찬","userId":"13376466808736477022"}}},"outputs":[],"source":["from src.model import SELayer, MLSTMfcn\n","from src.dataset import load_datasets\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F \n","import torch.optim as optim\n","import random\n","import os\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FSfWbY77Ddgq","outputId":"0ff310a9-94b3-418a-fe2c-c3b4260d4456"},"outputs":[{"name":"stdout","output_type":"stream","text":["mps\n"]}],"source":["# Seed and Device setting\n","\n","def def_seed(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","def_seed(seed=123)\n","\n","if torch.has_cuda:\n","    device = torch.device('cuda') \n","elif torch.has_mps:\n","    device = torch.device('mps')\n","else:\n","    device = torch.device('cpu')\n","print(device) # Print Using Device"]},{"cell_type":"markdown","metadata":{"id":"uQefObR0Ddgr"},"source":["### Tensor Structure\n","- shape : [NUM_DATA, MAX_LENGTH, NUM_FEATURE]\n","> 각 데이터별로(grid) time step에서의 state(value of each feature)을 1차원 텐서로, 모든 time step\n","> 핵심 : time step의 최대길이는 맞춤(존재하지 않는다면, 0으로 값을 주어 길이를 동일하게)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLVKZ-3CDdgs","outputId":"fd2f6c3c-c79c-43de-f91c-6f2b692bd807"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["def validation(model, testloader, criterion, device=device):\n","    accuracy = 0\n","    test_loss = 0\n","    for inputs, labels, seq_lens in testloader:\n","        inputs = inputs.float()\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        output = model.forward(inputs, seq_lens)\n","        test_loss += criterion(output, labels).item()\n","\n","        ## Calculating the accuracy \n","        # Model's output is log-softmax, take exponential to get the probabilities\n","        ps = torch.exp(output)\n","        # Class with highest probability is our predicted class, compare with true label\n","        equality = (labels.data == ps.max(1)[1])\n","        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n","        accuracy += equality.type_as(torch.FloatTensor()).mean()\n","\n","    return test_loss, accuracy\n","\n","\n","def train(model, trainloader, validloader, criterion, optimizer, \n","          epochs=10, print_every=10, device=device, run_name='model_mlstm_fcn'):\n","    print(\"Training started on device: {}\".format(device))\n","\n","    valid_loss_min = np.Inf # track change in validation loss\n","    steps = 0\n","    \n","    for e in range(epochs):\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        model.train()\n","        for inputs, labels, seq_lens in trainloader:\n","            steps += 1\n","\n","            inputs = inputs.float()\n","            inputs, labels = inputs.to(device),labels.to(device)\n","            \n","            optimizer.zero_grad()\n","            \n","            output = model.forward(inputs, seq_lens)\n","            loss = criterion(output, labels)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","\n","            if steps % print_every == 0:\n","                model.eval()\n","                \n","                with torch.no_grad():\n","                    valid_loss, accuracy = validation(model, validloader, criterion, device)\n","                \n","                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n","                      \"Training Loss: {:.6f}.. \".format(train_loss/print_every),\n","                      \"Val Loss: {:.6f}.. \".format(valid_loss/len(validloader)),\n","                      \"Val Accuracy: {:.2f}%\".format(accuracy/len(validloader)*100))\n","                \n","                # save model if validation loss has decreased\n","                if valid_loss <= valid_loss_min:\n","                    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","                    valid_loss_min,\n","                    valid_loss))\n","                    torch.save(model.state_dict(), 'weights/'+run_name+'.pt')\n","                    valid_loss_min = valid_loss\n","\n","                train_loss = 0\n","\n","                model.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHcJGltdDdgu"},"outputs":[],"source":["# Hyperparameters\n","epochs = 100\n","lr = 0.01\n","bs = 1024\n","# variable\n","NUM_CLASSES = 2\n","MAX_SEQ_LEN = 8640 # 24*365\n","NUM_FEATURES = 28"]},{"cell_type":"markdown","metadata":{"id":"ewtwsP58Ddgu"},"source":["## Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dwwwQSRDdgv"},"outputs":[],"source":["train_dataset, val_dataset, _ = load_datasets()\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = bs)\n","val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = bs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWOcVg7cDdgv","outputId":"8acaccf9-00a0-435f-bab5-d2a913d8f8fb"},"outputs":[{"data":{"text/plain":["2"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Train\n","mlstm_fcn_model = MLSTMfcn(\n","    num_classes=NUM_CLASSES,\n","    max_seq_len=MAX_SEQ_LEN,\n","    num_features=NUM_FEATURES\n",")\n","mlstm_fcn_model.to(device)\n","\n","optimizer = optim.Adam(mlstm_fcn_model.parameters(), lr=lr)\n","criterion = nn.NLLLoss()\n","\n","train(mlstm_fcn_model, train_loader, val_loader, criterion, optimizer, epochs, print_every=100, device=device, run_name=\"model_mlstm_fcn\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6QHcBhJDdgw"},"outputs":[],"source":["# Save model\n","torch.save(mlstm_fcn_model.state_dict())"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('torch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"c2819cb95bedbdbf0235ea4f18636776a6258e25e4398420083407874651edca"}},"colab":{"name":"train_test.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}